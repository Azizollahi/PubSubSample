Optimizing Device Status Storage: A Comparative Analysis of MongoDB Time-Series and PostgreSQL with GIN Indexes
1. Executive Summary
The proliferation of interconnected devices generates a continuous stream of status updates, presenting a significant data management challenge. Efficiently storing, querying, and analyzing this high-volume, time-stamped device status data requires database solutions optimized for rapid ingestion, flexible data retrieval, and scalable storage. This report provides an expert-level comparative analysis of two prominent approaches: MongoDB Time-Series collections and PostgreSQL utilizing Generalized Inverted Index (GIN) indexes, primarily with JSONB data types, for managing device status information.
MongoDB Time-Series collections represent a specialized feature within the MongoDB ecosystem, introduced in version 5.0, specifically engineered to enhance performance and reduce storage for time-series workloads. This is achieved through internal mechanisms like automatic data bucketing and columnar compression. Conversely, leveraging PostgreSQL with GIN indexes on JSONB columns offers a versatile strategy that combines the robustness and relational capabilities of a mature SQL database with efficient querying of semi-structured, time-stamped device status payloads.
The core findings of this analysis indicate that MongoDB Time-Series collections excel in scenarios prioritizing developer productivity within a document-centric data model and benefit from built-in optimizations tailored for time-series data, particularly for append-heavy workloads and flexible schema evolution. However, these collections come with notable limitations regarding updates, secondary indexing, and transactional support for writes. PostgreSQL, augmented with GIN indexes, provides strong ACID consistency, the power of SQL for complex queries and joins with other relational data, and highly flexible querying capabilities for attributes within JSONB payloads. The trade-offs include potential write overhead associated with GIN index maintenance and the absence of some native, advanced time-series features found in specialized databases, unless extensions like TimescaleDB are employed.
The fundamental difference between these two approaches extends beyond a mere feature-by-feature comparison. It touches upon the underlying data philosophy: MongoDB's document-oriented, schema-flexible paradigm contrasts with PostgreSQL's foundation in relational integrity, which has been enhanced to capably handle non-relational data formats like JSONB. This distinction significantly influences development workflows, data governance strategies, and the long-term interaction an organization has with its data. For instance, environments that prioritize rapid iteration and deal with highly dynamic data structures might find MongoDB's approach more aligned with their needs. In contrast, enterprises with complex, interconnected datasets and stringent consistency requirements might lean towards PostgreSQL.
Furthermore, the optimal solution is heavily contingent on the specific characteristics of the "device statuses" themselves—the complexity of their payloads, the frequency of structural changes, and the predominant query patterns. If device status payloads are deeply nested and exhibit high variability, MongoDB's native handling of such structures might offer a more straightforward development experience. However, if the payloads possess a relatively consistent core structure with variations, and if queries frequently target specific nested fields within these payloads, PostgreSQL's GIN indexes on JSONB columns can provide powerful and efficient access.
Ultimately, the recommendation is nuanced. This report guides the selection process by detailing the capabilities, performance profiles, and limitations of each solution, enabling a choice that aligns with specific workload characteristics, existing technological infrastructure, and overarching operational priorities. For many PostgreSQL users facing significant time-series demands, an extension like TimescaleDB may bridge the gap, offering specialized time-series capabilities on a familiar relational platform.
2. Understanding Device Status Data and Workload Characteristics
Effective selection of a database solution for device statuses necessitates a thorough understanding of the data's typical structure and the anticipated workload patterns. Device status data, by its nature, is time-series data, often originating from a multitude of sources such as IoT sensors, network equipment, or application components.
Typical Structure of Device Status Data
A typical device status record comprises several key elements:
	•	Timestamp: This is an essential component, marking the precise time an event or status was recorded (e.g., event_time, ts). The required precision of the timestamp (e.g., milliseconds, microseconds) can vary depending on the application's sensitivity to temporal changes. This field is critical for ordering, aggregation, and time-range queries.
	•	Device ID: A unique identifier (e.g., device_id, sensor_id, hostname) that distinguishes the source of the status update. This field is fundamental for correlating data to a specific device and is often used as the metaField in MongoDB Time-Series collections or as a primary key or indexed column in PostgreSQL. The cardinality (number of unique values) of the device_id field is a significant performance consideration for both database systems.
	•	Status Payload: This component typically contains the actual status information, often structured as a JSON or JSONB object. It can include a variety of metrics (e.g., temperature, cpu_utilization, memory_usage), states (e.g., is_online: true, current_mode: "active"), error codes, operational parameters, or even nested configuration objects. The complexity, size, and variability of this payload heavily influence data modeling choices and query strategies.
Common Query Patterns
The ways in which device status data is accessed and analyzed dictate the indexing and optimization requirements. Common query patterns include:
	•	Latest Status for One or All Devices: Frequently used for real-time dashboards, monitoring systems, and current state assessment. This requires efficient retrieval of the most recent record for each device or a specific device.
	•	Historical Status for a Specific Device Over a Time Range: Essential for trend analysis, debugging issues, and understanding device behavior over time. This involves filtering by device_id and a timestamp range.
	•	Filtering Devices by Status Attributes: Queries that search for devices matching specific criteria within their current or historical status payload (e.g., "find all devices with battery_level < 20%," or "identify devices that reported error_code: 'XYZ' in the last 24 hours"). These queries test the database's ability to efficiently search within semi-structured payloads.
	•	Aggregations: Calculating summary statistics such as average sensor readings, counts of online/offline devices, uptime percentages, or the occurrence rate of specific events. Aggregations are often performed over time windows and may be grouped by device attributes or other dimensions.
Key Workload Considerations
Beyond data structure and query types, several workload characteristics are crucial in determining the suitability of a database solution:
	•	Ingestion Rate: The volume of status updates received per unit of time (e.g., per second, minute). High ingestion rates necessitate databases optimized for write-intensive workloads and efficient indexing strategies.
	•	Data Volume: The total amount of storage required to retain device status data over its lifecycle. This impacts storage costs, backup strategies, and query performance on large historical datasets.
	•	Query Latency: The acceptable response time for different types of queries. Real-time monitoring may demand sub-second latency, while analytical queries on historical data might tolerate longer response times.
	•	Data Retention Policies: The duration for which data must be actively stored, and the procedures for archiving, downsampling, or deleting older data. Features like Time-To-Live (TTL) are relevant here.
	•	Schema Evolution: The frequency with which the structure of the status payload changes. If new metrics are often added or existing ones are modified, a database that handles schema changes gracefully is advantageous.
The complexity of the "status payload" itself is a pivotal factor in the decision-making process. A simple, flat payload with few key-value pairs might not fully leverage the advantages of MongoDB's document model or significantly stress PostgreSQL's JSONB capabilities. However, if the payloads are deeply nested, contain arrays, and exhibit high variability in their structure, the choice of database becomes more critical. In such cases, MongoDB's schema-on-read approach and its native query language (MQL) might offer more direct and potentially simpler manipulation of these complex documents. Conversely, while PostgreSQL's GIN indexes can be highly effective for querying specific paths within JSONB data, the queries themselves can become more complex, and performance may depend on how well the index is tailored to the specific access patterns.
Furthermore, the interplay between the cardinality of the Device ID and the common query patterns on payload attributes will significantly influence index design and overall performance in both systems. A high cardinality Device ID (i.e., a very large number of unique devices) is a known challenge for time-series databases. MongoDB typically uses this identifier as the metaField for its time-series collections, while in PostgreSQL, it would likely be indexed using a B-Tree for efficient lookups. If queries almost invariably filter by Device ID first, then the complexity of the payload and its indexing might be a secondary concern. However, if common queries involve scanning across many devices to find specific attributes within the payload (e.g., "find all devices of any type reporting a particular error state"), then the efficiency of indexing and querying the payload itself becomes paramount. This highlights a potential trade-off: optimizing primarily for device-centric queries versus optimizing for payload-attribute-centric queries.
3. MongoDB Time-Series Collections for Device Statuses
MongoDB introduced time-series collections in version 5.0 as a specialized mechanism to efficiently store and query sequences of measurements over time. These collections are not merely regular collections with a timestamp field; they employ specific internal optimizations designed for time-series workloads.
Core Concepts
Understanding the following core concepts is crucial for effectively utilizing MongoDB Time-Series collections:
	•	timeField: This is a mandatory field in the documents inserted into a time-series collection, and it must be of a BSON date type. MongoDB internally organizes and sorts the data primarily based on this field. The choice of timestamp precision (e.g., seconds, milliseconds) should align with the granularity of the incoming device status updates.
	•	metaField: This optional field specifies which field in the documents contains metadata that uniquely identifies a particular time series. For device statuses, this would typically be the device_id or a composite object if a single device reports multiple distinct series (e.g., { "deviceId": "sensor-001", "metric_type": "temperature" }). Documents sharing the same metaField value are grouped together by MongoDB, which is fundamental to its bucketing strategy and query optimization. The value of the metaField should rarely change for a given series.
	•	granularity: This parameter, configured during collection creation, dictates the approximate time span covered by each internal data bucket (e.g., seconds, minutes, hours). The chosen granularity influences how data is physically grouped and can affect storage density and the performance of queries that scan time ranges. A finer granularity might lead to more, smaller buckets, especially if the metaField also has high cardinality.
	•	Bucketing Mechanism: Internally, MongoDB optimizes time-series data by grouping documents into "buckets" based on the timeField, metaField, and granularity settings. These buckets are not directly exposed to the user but are stored in an optimized, columnar format within special underlying collections (prefixed with system.buckets.). This columnar storage within buckets allows for better compression and reduced disk I/O during queries. Developers interact with the logical time-series collection as if it were a regular collection, while MongoDB handles the bucketing and un-bucketing transparently.
Data Modeling Strategies for Device Statuses
For device status data, common modeling approaches include:
	•	Using a single device_id field as the metaField. This is suitable when each device reports a single stream of status updates.
	•	Employing a composite metaField if a device has multiple distinct sensors or components reporting their own status series (e.g., metaField: { device_id: "XYZ", sensor_name: "CPU_TEMP" }).
	•	The actual status payload (e.g., temperature, battery level, error messages) would be stored as top-level fields within the same document that contains the timeField and metaField.
Performance Profile
Ingestion: MongoDB Time-Series collections are designed for append-heavy workloads, which are characteristic of device status updates. For optimal ingestion performance, MongoDB recommends batching inserts. Interestingly, grouping these batches by metadata (i.e., by the metaField values) rather than strictly by time can sometimes be more efficient, as it helps MongoDB organize data into the correct buckets more effectively. While time-series collections aim to simplify ingestion, some benchmarks comparing MongoDB with specialized time-series databases like QuestDB or TimescaleDB have shown MongoDB to have slower ingestion rates under certain conditions. Ingestion performance can also be sensitive to the order of fields within the document if not structured correctly, though this is less of a concern with the dedicated time-series collection type compared to manual time-series patterns in regular collections.
Query Performance: Queries filtering on the timeField and metaField are generally efficient due to the clustered index automatically created on these fields (or their internal representations within the bucketing structure). MongoDB's query engine performs optimizations such as query rewrites and leverages pre-computed metadata about buckets (like min/max time values) to quickly identify relevant buckets for a given query. Secondary indexes can only be created on the metaField (or its sub-fields), which further aids in locating specific series. Performance for simple aggregations can be quite good. However, for more complex analytical queries, MongoDB might be outperformed by databases with more mature SQL optimizers or specialized analytical engines. It's important to note that the distinct command is inefficient on time-series collections due to their internal data structure; using a $group stage in an aggregation pipeline is the recommended alternative for finding unique values.
Impact of metaField Cardinality: The cardinality of the metaField (e.g., the number of unique device IDs) is a critical factor. A very high cardinality can lead to the creation of a large number of small, potentially sparsely packed buckets. This can, in turn, decrease storage efficiency (due to overhead per bucket) and query efficiency (as more buckets might need to be examined or managed by the bucket catalog). While MongoDB is designed to handle many distinct time series, extreme cardinality can still place a strain on in-memory resources used for managing the bucket catalog and for query planning. Nevertheless, practical experience reported by users suggests that queries filtered by specific metaField values (such as individual sensor IDs) tend to perform well.
The metaField is a crucial element for performance but also introduces significant constraints. Its central role in bucketing, indexing (it's the only field allowing secondary indexes), and sharding is beneficial when queries align with it. However, if there's a frequent need to query status attributes that are not part of the metaField and without first filtering by the metaField (e.g., finding all devices, regardless of ID, that have payload.some_attribute > X), these queries cannot leverage the primary time-series indexing structure effectively. This could lead to slower performance as MongoDB might need to scan numerous buckets. The alternative, pulling frequently queried static attributes into the metaField itself, could make the metaField more complex and potentially increase its cardinality, which has its own performance implications.
Storage Efficiency
MongoDB Time-Series collections employ best-in-class columnar compression algorithms within the data buckets. This, combined with data densification (packing data points closely together), can significantly reduce the storage footprint compared to storing the same data in regular MongoDB collections. The actual compression ratio achieved depends on the nature of the data (e.g., data types, value ranges, repetition).
Specialized Time-Series Features
	•	TTL (Time-To-Live): Collections can be configured with an expireAfterSeconds parameter, enabling automatic deletion of documents older than a specified duration. This is useful for managing data retention policies without manual intervention.
	•	Window Functions & Temporal Operators: The MongoDB aggregation framework provides powerful tools for time-series analysis, including window functions (via $setWindowFields) and various temporal operators (e.g., $dateTrunc, $derivative, $integral) for tasks like calculating moving averages, time-based grouping, and rates of change.
Limitations and Operational Considerations
MongoDB Time-Series collections come with a specific set of limitations:
	•	Updates: Document updates are highly restricted. An update operation can only match on the metaField's value and can only modify the metaField's value. The update document must only contain update operator expressions, must not limit the number of documents to update (i.e., multi: true or updateMany() must be used), and cannot set upsert: true. This means status payloads themselves are effectively immutable once written.
	•	Secondary Indexes: As mentioned, secondary indexes are only permitted on the metaField (or its sub-fields). Multikey, 2dsphere (for geospatial data on metaField), and sparse indexes are supported on the metaField. However, text indexes and unique indexes (other than the internal _id) are not supported on the time-series collection itself.
	•	Sharding: Time-series collections can be sharded. However, the shard key must consist of the metaField (or its sub-fields) and/or the timeField. The _id field cannot be part of the shard key pattern. Zone sharding is not supported for time-series collections; data is always distributed evenly.
	•	Unsupported Features: A significant number of standard MongoDB features are not supported with time-series collections. These include Atlas Search, Change Streams (for real-time data capture), Client-Side Field Level Encryption, Database Triggers, schema validation rules applied directly to the time-series collection, the reIndex command, and the renameCollection command.
	•	Transactions: Writes to time-series collections cannot be part of multi-document ACID transactions. Read operations from time-series collections, however, can be included in transactions.
	•	Maximum Document Size: The maximum BSON document size for documents within a time-series collection is 4 MB.
	•	Collection Type Conversion: An existing regular collection cannot be converted into a time-series collection, nor can a time-series collection be converted into a regular collection type. Data migration is required for such transformations.
The extensive list of unsupported features underscores that a MongoDB Time-Series collection is not simply a general-purpose MongoDB collection with added time-series optimizations. Instead, it is a highly specialized collection type with considerable trade-offs. The absence of features like Change Streams or Database Triggers means that use cases requiring real-time reactions to device status changes (e.g., immediate alerting) cannot be implemented directly on the time-series collection data itself. Workarounds, such as application-level logic feeding another collection or periodic batch processing, would be necessary, potentially adding architectural complexity. This positions MongoDB Time-Series collections as being optimally suited for specific data shapes and access patterns but less flexible if these patterns need to diverge significantly from what the specialized collection type supports.
While MongoDB often promotes ease of use and developer productivity, achieving optimal performance with time-series data, even using the dedicated collections, involves careful consideration of schema design. The choices for metaField and granularity directly impact storage and query performance. This requires a thoughtful analysis of the data characteristics and query patterns, which is a form of schema design, despite MongoDB's reputation for being "schema-less." Benchmarks comparing "Mongo-naive" (simple document-per-event) versus "Mongo-recommended" (more structured, often pre-aggregated or bucketed approaches, even before official TS collections) have shown that optimal performance isn't always achieved with the simplest approach. Thus, expertise and planning are still necessary to maximize the benefits of MongoDB Time-Series collections.
4. PostgreSQL with GIN Indexes for Device Statuses
PostgreSQL, a powerful open-source object-relational database system, can be adapted to store and manage device status data, particularly by leveraging its JSONB data type for flexible payloads and GIN indexes for efficient querying of this semi-structured information.
Core Concepts
	•	JSONB Data Type: PostgreSQL offers two JSON data types: json and jsonb. For most applications involving querying and indexing, jsonb is the preferred choice. It stores JSON data in a decomposed binary format, which is slightly slower to input due to conversion overhead but significantly faster to process and query. jsonb also removes insignificant whitespace, sorts keys, and only stores the last instance of any duplicate keys within an object. This makes it highly efficient for storing device status payloads that might have varying attributes.
	•	GIN (Generalized Inverted Index): GIN indexes are designed for indexing "composite" values, where a single indexed item (like a JSONB document or an array) can contain multiple searchable elements (like keys or values within the JSONB document). For a JSONB column, a GIN index creates an inverted list, mapping individual keys, values, or key-value pairs from the JSONB documents back to the rows that contain them. This allows for fast searching of rows based on the content of the JSONB data.
	•	Relevant Operator Classes for JSONB: When creating a GIN index on a JSONB column, specific operator classes determine how the data is indexed and which operators can use the index:
	•	jsonb_ops (default): This operator class indexes every key and every value within the JSONB document. It supports a wide range of operators, including the containment operator (@>), key existence operators (?, ?|, ?&), and others.
	•	jsonb_path_ops: This operator class is more specialized. It supports fewer operators (primarily @>) but can offer better performance for these specific containment queries by indexing hash values of JSON paths and values. The choice between jsonb_ops and jsonb_path_ops depends on the predominant query patterns. jsonb_ops is more versatile, while jsonb_path_ops is optimized for specific containment checks.
Data Modeling Strategies for Device Statuses
A common approach for storing device statuses in PostgreSQL would involve a table structure similar to the following:
	•	A timestamp column, typically of type TIMESTAMPTZ (timestamp with time zone), to store the event time. This column would usually have a B-Tree index, possibly as part of a composite index.
	•	A device_id column, using a type like TEXT, VARCHAR, or UUID, to uniquely identify the device. This column would also be indexed, typically with a B-Tree index, for fast lookups.
	•	A status_payload column of type JSONB, which would store the variable device status attributes. This column would be indexed using a GIN index.
Optionally, if certain attributes within the JSONB payload are frequently queried, have a fixed structure, and are critical for performance, they might be extracted into separate, regular columns. These regular columns could then benefit from standard B-Tree indexing, potentially offering faster lookups for those specific fields than querying within the JSONB structure, though this reduces the flexibility of the JSONB payload.
Performance Profile
Ingestion: Creating and maintaining GIN indexes incurs a higher overhead on write operations (INSERT, UPDATE) compared to simpler B-Tree indexes. This is because indexing a single JSONB document can result in many individual entries in the GIN index (one for each indexed key/value). PostgreSQL employs a mechanism called fastupdate for GIN indexes by default, which defers some index updates by batching them in a temporary pending list. This improves initial write performance, but the pending list must eventually be flushed to the main index structure, which can cause periodic slowdowns. Over-indexing, in general, is detrimental to write performance as every index needs to be updated upon data modification. Some benchmarks indicate that native PostgreSQL (without specialized time-series extensions) can be slower on ingestion compared to databases specifically designed for time-series data or even MongoDB under certain conditions.
Query Performance: GIN indexes can dramatically accelerate queries that filter on attributes within JSONB columns. For example, a query like SELECT * FROM device_statuses WHERE status_payload @> '{"state": "ERROR", "code": 123}'; can efficiently use a GIN index on the status_payload column. GIN indexes typically lead to bitmap index scans, where the index identifies all matching rows, and then these rows are fetched from the table. When combined with standard B-Tree indexes on timestamp and device_id columns, PostgreSQL can execute complex queries that filter by time range, specific devices, and particular attributes within the status payload. The performance of querying JSONB data depends on the complexity of the JSON structure itself, the nature of the query, and the effectiveness of the GIN index and operator class used. PostgreSQL's mature and sophisticated query optimizer is generally adept at handling complex SQL queries.
Impact of High Cardinality on Indexed Columns: For a device_id column with high cardinality indexed by a B-Tree, PostgreSQL generally handles equality lookups very well. If there's high cardinality within the JSONB fields themselves (i.e., many unique keys or many unique values across different documents), this can lead to larger GIN indexes. General concerns related to high cardinality in any database system, such as increased memory usage for operations (governed by work_mem) and a higher load on the autovacuum process for table and index maintenance, also apply to PostgreSQL.
Using PostgreSQL with GIN indexes on JSONB for device statuses essentially means constructing a custom time-series solution upon a general-purpose relational database. This approach offers considerable flexibility but also shifts a significant portion of the optimization burden—such as implementing partitioning strategies for large tables, managing data retention, and creating specialized time-based aggregations—to the developer or database administrator. This is a key area where extensions like TimescaleDB provide substantial value by automating or simplifying these complex tasks. Without such extensions, a "vanilla" PostgreSQL setup for high-volume time-series data might eventually encounter performance and manageability challenges related to very large tables, index bloat, and inefficient time-based analytical queries unless meticulous manual partitioning and aggressive vacuuming strategies are implemented.
Storage Considerations for JSONB and GIN Indexes
JSONB, being a binary format, is generally more compact for storing JSON data than a plain text representation. However, GIN indexes themselves can become quite large, as they need to store entries for the various keys and/or values extracted from the indexed JSONB documents. PostgreSQL uses its standard TOAST (The Oversized Attribute Storage Technique) mechanism for handling large JSONB objects that exceed a certain size, but it does not offer the same kind of built-in, specialized columnar compression for general table data that is found in MongoDB Time-Series collections or PostgreSQL extensions like TimescaleDB. The latter's compression capabilities can lead to significant storage savings for time-series data.
Leveraging General PostgreSQL Strengths
Choosing PostgreSQL for device status management allows applications to benefit from its core strengths:
	•	ACID Compliance: PostgreSQL provides full Atomicity, Consistency, Isolation, and Durability (ACID) guarantees for all transactions, ensuring high data integrity and reliability.
	•	Relational Integrity and Joins: The ability to define relationships between tables using foreign keys and to perform complex SQL joins with other relational data (e.g., device master tables, user information, maintenance logs) is a significant advantage.
	•	Advanced SQL Capabilities: PostgreSQL supports a rich SQL dialect, including advanced window functions, Common Table Expressions (CTEs), stored procedures (in various languages like PL/pgSQL, PL/Python), and user-defined functions and types, offering immense flexibility for data manipulation and analysis.
	•	Mature and Extensible Ecosystem: PostgreSQL boasts a vast and mature ecosystem of client libraries, Object-Relational Mappers (ORMs), administration tools, and a wide array of extensions (like PostGIS for geospatial data or pgvector for similarity search) that can enhance its capabilities.
The "impedance mismatch" of querying deeply nested or highly variable JSONB attributes using SQL, even with the assistance of GIN indexes, can sometimes lead to complex and verbose queries. While PostgreSQL's JSONB operators (->, ->>, @>, etc.) are powerful, constructing SQL queries to navigate and filter on multiple, potentially absent, nested fields can be more cumbersome and harder to maintain compared to a native document query language like MongoDB's MQL for similar tasks. Although GIN indexes are designed to accelerate these queries, the query optimizer's ability to efficiently utilize them for very complex path expressions or combinations of conditions within the JSONB might vary. This could necessitate careful query tuning or even adjustments to the JSONB structure to optimize specific access patterns.
However, a crucial advantage of the PostgreSQL approach lies not just in its JSONB/GIN capabilities for the status payload, but in its inherent ability to seamlessly integrate this time-series device status data with other critical relational datasets. Device statuses rarely exist in isolation; they often relate to detailed device inventory specifications, customer account information, operational logs, or maintenance schedules stored in other relational tables. The capacity to perform a single, transactionally consistent SQL query that joins historical device status data (from the table with JSONB and GIN indexes) with, for example, device manufacturing details (from a devices table) and user subscription information (from a users table) is a powerful capability. This is a strength that MongoDB, particularly its Time-Series collections with their transactional limitations on writes, cannot easily match without resorting to application-level joins or extensive data denormalization, which can introduce their own complexities and consistency challenges.
Limitations for Native Time-Series Workloads and the Role of Extensions
While PostgreSQL is a highly capable general-purpose database, its native feature set lacks some of the specialized optimizations found in purpose-built time-series databases. For instance, vanilla PostgreSQL does not offer:
	•	Automatic time-based partitioning of tables.
	•	Specialized columnar compression tailored for time-series data.
	•	A rich set of out-of-the-box advanced time-series analytical functions (beyond standard SQL window functions).
	•	Optimized data retention policies beyond what can be achieved with custom scripting or triggers for TTL-like behavior.
This is precisely where PostgreSQL extensions like TimescaleDB play a critical role. TimescaleDB builds upon PostgreSQL, transforming it into a powerful time-series database by adding these missing specialized capabilities, such as hypertables (automatic time-space partitioning), native compression, and a suite of time-series-specific functions.
5. Comparative Analysis: MongoDB Time-Series vs. PostgreSQL GIN
This section provides a direct comparison of MongoDB Time-Series collections and PostgreSQL with GIN indexes for managing device status data, focusing on key operational and performance dimensions.
Data Ingestion Performance:
	•	MongoDB Time-Series: Generally designed for high-throughput append-heavy workloads, characteristic of time-series data. Optimal performance often relies on batching inserts and strategically grouping data by the metaField. However, independent benchmarks comparing MongoDB with specialized time-series databases like QuestDB and TimescaleDB have indicated that MongoDB can exhibit slower ingestion rates under certain test conditions.
	•	PostgreSQL + GIN: The maintenance of GIN indexes, especially on complex JSONB payloads, introduces write overhead. Each insert or update to an indexed JSONB column might require multiple updates to the GIN index structure. PostgreSQL's fastupdate mechanism for GIN helps by batching some of these updates, but the fundamental cost remains. Consequently, for raw ingestion speed, particularly with intricate JSONB structures requiring extensive GIN indexing, native PostgreSQL is likely to be slower than a system specifically optimized for time-series writes or even MongoDB Time-Series collections. One benchmark noted PostgreSQL being slower on insert and update operations compared to MongoDB in a specific scenario.
For raw ingestion speed of relatively simple time-series data, MongoDB's specialized Time-Series collections might hold an advantage over PostgreSQL with GIN indexes due to the inherent optimizations for this data pattern in MongoDB versus the more general-purpose nature of GIN indexing in PostgreSQL. The overhead of GIN indexing each JSONB document, which could involve numerous keys and values, represents a fundamental cost during data insertion. However, actual performance is highly dependent on the specific data characteristics, hardware, and batching strategies employed.
Query Performance:
	•	Latest Status (per device/all devices):
	•	MongoDB TS: Efficient retrieval is possible using the $last accumulator within a $group stage (if needing the last for many devices) or a find() operation with appropriate sorting on the timeField and limiting, especially when filtered by the metaField.
	•	PostgreSQL+GIN: Achievable efficiently using a query like SELECT DISTINCT ON (device_id) * FROM device_statuses ORDER BY device_id, timestamp DESC;. This pattern performs well with a B-Tree index on (device_id, timestamp).
	•	Status History (specific device, time range):
	•	MongoDB TS: Queries are fast when filtering by the metaField (e.g., device_id) and the timeField range, as these are the primary indexed fields.
	•	PostgreSQL+GIN: Similarly fast with a B-Tree index on (device_id, timestamp) for filtering by device and time range.
	•	Filtering by Complex JSON Attributes:
	•	MongoDB TS: Querying on attributes within the status payload that are not part of the metaField is limited by the lack of secondary indexing capabilities beyond the metaField. Such queries might require scanning data within buckets if the metaField cannot be used to narrow down the search space significantly.
	•	PostgreSQL+GIN: This is a strong suit for PostgreSQL. GIN indexes, using operator classes like jsonb_ops or jsonb_path_ops, are specifically designed to efficiently query elements within JSONB documents. This allows for powerful and flexible filtering based on various attributes nested within the status payload.
	•	Aggregations (grouped by time and/or device attributes):
	•	MongoDB TS: The aggregation framework, including window functions and temporal operators, supports complex time-series aggregations. However, benchmark results for complex aggregations have been mixed when compared to other systems.
	•	PostgreSQL+GIN: Offers the full power and expressiveness of SQL for aggregations, including GROUP BY clauses and advanced window functions. PostgreSQL's mature query optimizer is generally effective at handling such queries.
PostgreSQL combined with GIN indexes likely provides more versatile and potentially more performant querying when complex filters on diverse payload attributes are required, owing to the specific design of GIN indexes for searching within composite types. MongoDB Time-Series collections excel when queries align well with the timeField and metaField, which are the primary axes of optimization.
Storage Efficiency:
	•	MongoDB Time-Series: Employs columnar compression algorithms within its internal buckets and data densification techniques. This can lead to substantial storage savings for time-series data compared to regular MongoDB collections.
	•	PostgreSQL + GIN: The JSONB data type itself is a binary format, offering some compactness over plain text JSON. However, GIN indexes can be relatively large, as they need to store entries for the various keys and values within the indexed JSONB documents. Native PostgreSQL (without extensions like TimescaleDB) does not feature the same kind of specialized columnar compression for general table data as seen in MongoDB Time-Series collections. TimescaleDB, if used as an extension, introduces highly effective columnar compression for time-series data, which can dramatically reduce storage footprint.
Out-of-the-box, MongoDB Time-Series collections are likely to offer better storage efficiency for time-series data due to their specialized internal columnar compression mechanisms. PostgreSQL's standard TOAST mechanism is designed for handling large individual objects rather than providing optimized compression across many rows of time-series data.
Scalability (Data Volume, Number of Devices, Query Load):
	•	MongoDB Time-Series: Designed for horizontal scalability through sharding. Specific restrictions apply to the shard key for time-series collections, which must be based on the metaField, its sub-fields, and/or the timeField.
	•	PostgreSQL + GIN: Typically scales vertically (i.e., by increasing the resources of a single server). Horizontal scaling (sharding) in PostgreSQL is more complex to implement, often requiring third-party solutions or significant manual configuration. Extensions like TimescaleDB enhance PostgreSQL's scalability for time-series workloads, including features for distributed hypertables.
MongoDB generally provides more straightforward built-in mechanisms for horizontal scalability, particularly for high data volumes and write-intensive loads, although its time-series sharding has specific constraints. PostgreSQL's primary scaling model is vertical, with horizontal scaling being a more involved endeavor.
Data Modeling Flexibility vs. Rigidity:
	•	MongoDB Time-Series: Offers flexibility for the structure of the status payload itself, as it's stored in BSON (Binary JSON). However, the timeField and metaField definitions are fixed after the collection is created and cannot be modified.
	•	PostgreSQL + GIN: The JSONB data type allows for flexible and evolving status payloads. The overall table schema (e.g., columns for device_id, timestamp) is predefined and more rigid, adhering to relational database principles.
Both solutions offer flexibility for the device status payload through JSON/BSON. MongoDB provides greater overall schema flexibility at the collection level (schema-on-read), while PostgreSQL enforces a defined structure for its non-JSONB columns (schema-on-write).
Development and Operational Complexity:
	•	MongoDB Time-Series: May appear simpler for developers already familiar with MongoDB's document model and query language. However, the specific limitations of time-series collections (e.g., restrictions on updates, secondary indexing, and the list of unsupported features) introduce operational nuances and require careful consideration during design.
	•	PostgreSQL + GIN: Operations for a mature RDBMS like PostgreSQL are generally well-understood. However, managing very large tables, potential GIN index bloat, and optimizing autovacuum settings require specific DBA skills and attention. The SQL syntax for querying JSONB can also become complex for deeply nested structures.
The perception of "complexity" is subjective. MongoDB might offer a faster start for teams comfortable with its ecosystem, but PostgreSQL's operational maturity is a known quantity. Both solutions demand a learning curve for optimal deployment and management in a demanding time-series scenario.
Handling High Cardinality Dimensions (e.g., device_id):
	•	MongoDB Time-Series: A high cardinality metaField can lead to a large number of small internal buckets, which may impact storage and query efficiency. However, MongoDB is generally designed to handle a large number of distinct time series.
	•	PostgreSQL + GIN: A B-Tree index on a high-cardinality device_id column typically performs well for direct lookups. The size of GIN indexes on JSONB can grow with the overall data volume and the number of unique elements within the JSONB. Autovacuum processes on tables with high-cardinality indexes may require careful tuning to manage maintenance overhead effectively.
Both systems have mechanisms to cope with high cardinality, but extreme levels will stress either. The nature of this stress differs: it relates more to bucket management and the in-memory bucket catalog in MongoDB, versus index size, maintenance, and autovacuum overhead in PostgreSQL.
Data Consistency and Transactional Guarantees:
	•	MongoDB Time-Series: Does not support transactional writes to time-series collections. Writes are atomic at the document level. In sharded environments, MongoDB offers eventual consistency by default for reads across shards, though stronger consistency can be configured.
	•	PostgreSQL + GIN: Provides full ACID compliance for all operations, ensuring strong consistency and reliable transaction processing across the entire database, including tables storing device statuses and any related relational data.
PostgreSQL offers unequivocally stronger and more comprehensive data consistency guarantees due to its relational foundation and full ACID compliance for all operations. The lack of write transaction support for MongoDB Time-Series collections is a key differentiator.
Ecosystem and Tooling:
	•	MongoDB Time-Series: Leverages the broader MongoDB ecosystem, including MongoDB Atlas (its managed cloud service), Compass (GUI), Mongoose (ODM for Node.js), and various drivers.
	•	PostgreSQL + GIN: Benefits from a vast and mature ecosystem, with a wide range of client libraries for numerous programming languages, diverse ORMs, sophisticated administration tools, and a rich collection of extensions.
Both databases possess strong and active ecosystems. PostgreSQL's ecosystem is arguably broader and has a longer history, characterized by its open nature and extensibility. MongoDB Atlas provides a highly polished and feature-rich managed database service.
The following tables summarize the comparative aspects:
Table 1: Feature and Capability Matrix (MongoDB TS vs. PostgreSQL+GIN)
Feature/Capability
MongoDB Time-Series
PostgreSQL + GIN
Ingestion Rate
Optimized for append; batching by metaField key. Benchmarks vary vs specialized TSDBs.
GIN index write overhead. fastupdate helps. May be slower for complex JSONB.
Query (Simple Time/Device)
Efficient via timeField & metaField indexing.
Efficient via B-Tree on timestamp & device ID.
Query (Complex JSON Attributes)
Limited by metaField-only secondary indexing.
Powerful via GIN on JSONB (jsonb_ops, jsonb_path_ops).
Query (Aggregations)
Aggregation framework with window functions. Performance mixed for complex cases.
Full SQL power, mature optimizer, window functions.
Storage Efficiency
Columnar compression in buckets, densification. Good savings.
JSONB binary format. GIN indexes can be large. No native TS columnar compression.
Scalability (Horizontal)
Built-in sharding ( metaField/timeField key).
Complex, often via 3rd party tools or manual setup. TimescaleDB enhances this.
Scalability (Vertical)
Possible via instance sizing.
Primary scaling method for native PostgreSQL.
Schema Flexibility (Payload)
High (BSON documents).
High (JSONB data type).
Schema Flexibility (Overall)
High (collection level). timeField/metaField fixed.
Table structure predefined; JSONB flexible.
Transaction Support (Writes)
No for TS collections. Document-level atomicity.
Full ACID for all operations.
Indexing Flexibility (Payload)
Limited to metaField for secondary indexes.
High via GIN on JSONB; B-Tree on extracted columns.
High Cardinality Handling
metaField cardinality impacts bucketing. Designed for many series.
B-Tree good for ID lookups. GIN size/maintenance needs care. Autovacuum tuning.
Specialized Time-Series Func.
Aggregation framework (e.g., $derivative, $setWindowFields).
Native SQL window functions. TimescaleDB adds many more.
TTL / Data Retention
expireAfterSeconds parameter.
Manual (e.g., scheduled deletes, partitioning) or via TimescaleDB policies.
Operational Overhead
Specific TS limitations. Atlas simplifies. Learning curve for optimal use.
Mature RDBMS ops. GIN bloat, vacuuming. JSONB query tuning. Learning curve.
Ecosystem
Strong MongoDB ecosystem, Atlas.
Vast, mature, many extensions.
Table 2: Summary of Key Limitations and Trade-offs
Solution
Limitation/Trade-off
Implication for Device Status Workload
Relevant References
MongoDB TS
Update restrictions (only metaField)
Status payloads are effectively immutable post-ingestion. Cannot easily correct historical payload data.


Secondary index limitations (only on metaField)
Poor performance for queries filtering on payload attributes not in metaField, without also filtering by metaField.


No transaction support for writes to TS collections
Cannot guarantee atomicity for operations involving multiple device status writes or writes linked with other collections.


Unsupported features (Atlas Search, Change Streams, Triggers, Schema Validation etc.)
Limits integration options and real-time processing directly on TS data (e.g., no Change Streams for immediate alerts).

PostgreSQL+GIN
GIN index write overhead
Slower ingestion rates, especially with frequent updates or complex JSONB structures leading to many GIN entries.


Lack of native advanced TS features (partitioning, columnar compression)
Requires manual implementation for managing large tables (e.g., partitioning) and achieving optimal storage for time-series.


Potential GIN index bloat and maintenance
Requires diligent autovacuum tuning and possible REINDEX operations to maintain performance and control storage.


SQL complexity for deeply nested JSONB
Queries on complex, variable JSONB structures can become verbose and harder to optimize compared to native document queries.

These tables provide a structured overview of the capabilities and constraints, aiding in a balanced assessment of each solution for the specific needs of managing device status data.
6. In-Depth Elaboration, Multi-Layered Insights, and Detailed Recommendations
The choice between MongoDB Time-Series collections and PostgreSQL with GIN indexes for storing device statuses is not straightforward and depends heavily on the specific context, priorities, and technical environment of the application. This section delves deeper into scenario-based recommendations, addresses specific user priorities, considers the role of extensions like TimescaleDB, and provides guidance for prototyping.
Scenario-Based Recommendations
Choose MongoDB Time-Series collections if:
	•	Existing MongoDB Ecosystem and Expertise: The organization already has a significant investment in MongoDB infrastructure, and the development and operations teams possess strong MongoDB expertise. Leveraging existing knowledge and tools can reduce the learning curve and operational friction.
	•	Document-Centric Application Model: The application's data model is inherently document-centric, and device status payloads are naturally represented as BSON/JSON documents that may evolve frequently in structure. MongoDB's flexibility in handling such dynamic schemas is a key advantage.
	•	Prioritization of Development Velocity and Schema Flexibility: Rapid iteration cycles and the ability to adapt to changing data structures without formal schema migrations are paramount.
	•	Requirement for Built-in Horizontal Scalability for Writes: The application anticipates very high write volumes that necessitate horizontal scaling, and MongoDB's native sharding capabilities (though with specific constraints for time-series collections) are preferred over more complex third-party or manual sharding solutions.
	•	Alignment with timeField and metaField Querying: The predominant query patterns align well with filtering and aggregating based on the timeField and metaField. The limitations on secondary indexing (only on metaField) and the restricted update capabilities are considered acceptable trade-offs for the specialized time-series optimizations.
Choose PostgreSQL with GIN Indexes if:
	•	Non-Negotiable ACID Compliance and Transactional Integrity: Strong ACID guarantees across all data, including device statuses and any related relational data (e.g., device inventory, user accounts), are critical for the application's reliability and data consistency.
	•	Frequent Need for Complex SQL Queries and Joins: The application requires frequent and complex analysis involving SQL queries that join device status data with other relational tables. PostgreSQL's mature SQL engine and relational capabilities are essential for these operations.
	•	Criticality of Flexible and Powerful Indexing on JSONB Payload Attributes: The ability to efficiently query and filter based on various attributes, including those deeply nested within the JSONB status payload, is a primary requirement. GIN indexes provide this capability.
	•	Existing PostgreSQL Infrastructure or Preference for Mature RDBMS: The organization has an established PostgreSQL infrastructure, or there is a strong preference for the robustness, maturity, and comprehensive feature set of a traditional RDBMS.
	•	Preparedness for RDBMS Operational Demands or Consideration of Extensions: The team is equipped to manage potential GIN index write overhead, large table maintenance (including partitioning and vacuuming strategies), or is open to using PostgreSQL extensions like TimescaleDB to enhance capabilities for very large-scale time-series workloads.
The decision is also influenced by the organization's maturity and its overarching data strategy. A greenfield project with a small, agile team might favor MongoDB for its rapid prototyping capabilities. In contrast, an established enterprise with complex existing data systems and stringent governance requirements might find PostgreSQL's structured approach, even when handling JSONB, more suitable. The perceived "cost of change" or the investment required to build new expertise is a practical, though often unstated, factor that influences technology choices.
Addressing Specific User Priorities
	•	Write-Heavy Workloads: MongoDB Time-Series collections are specifically designed for high-volume, append-mostly writes. However, benchmarks have shown variable performance compared to other specialized systems. Effective batching is crucial. PostgreSQL with GIN indexes will experience write overhead due to index maintenance. For either system, appropriate batching strategies are recommended to optimize ingestion.
	•	Read-Heavy Workloads (Complex Queries on Payload): PostgreSQL with GIN indexes is likely to offer superior performance and flexibility here, as GIN indexes are explicitly designed for searching within composite data types like JSONB. MongoDB Time-Series collections are limited by their secondary indexing capabilities, which are restricted to the metaField.
	•	Read-Heavy Workloads (Time/Device-Centric Queries): Both solutions can be highly efficient for queries that primarily filter by device ID and time range, as these access patterns align with their primary indexing strategies.
	•	Operational Simplicity: This is subjective. MongoDB Atlas provides a polished managed service that can simplify operations. However, MongoDB Time-Series collections have their own specific operational considerations and limitations. PostgreSQL is a mature RDBMS with well-understood operational patterns, but managing it at a very large scale, especially with GIN indexes and large JSONB datasets, requires significant DBA expertise (e.g., for vacuum tuning, partitioning, index maintenance).
	•	Cost: Both MongoDB Community Edition and PostgreSQL are open-source, mitigating software licensing costs. Cloud hosting costs will vary based on the provider and resource consumption. MongoDB's storage efficiency through columnar compression in Time-Series collections can positively impact storage costs. For PostgreSQL, achieving similar storage efficiency for time-series data typically requires an extension like TimescaleDB, which has its own licensing or service costs if using a managed version.
The TimescaleDB Consideration
If the requirements lean towards PostgreSQL due to its relational strengths, ACID compliance, and powerful SQL, but the application involves very high-volume time-series data demanding specialized features, TimescaleDB should be seriously evaluated. TimescaleDB is an extension for PostgreSQL that effectively transforms it into a purpose-built time-series database.
Key advantages of TimescaleDB for this use case include :
	•	Automatic Time/Space Partitioning: TimescaleDB introduces "hypertables," which automatically partition data by time and optionally other dimensions (like device_id) into "chunks." This significantly simplifies the management of large time-series tables and improves query performance by allowing the query planner to prune irrelevant chunks.
	•	Columnar Compression: TimescaleDB offers native columnar compression for its chunks, which can dramatically reduce storage footprint (often by 90% or more for time-series data) and improve query speed for analytical workloads by reducing I/O.
	•	Specialized Time-Series Functions: It provides a rich library of SQL functions optimized for time-series analysis (e.g., time_bucket, first, last, gap-filling functions).
	•	Improved Data Retention Policies: TimescaleDB facilitates the setup of automated policies for dropping or compressing old data chunks.
	•	Enhanced Scalability: While built on PostgreSQL, TimescaleDB incorporates optimizations that improve scalability for time-series workloads, including options for distributed hypertables in multi-node setups.
Benchmarks frequently demonstrate TimescaleDB outperforming both native PostgreSQL and MongoDB for demanding time-series workloads in terms of ingestion rates, query speeds (especially for complex analytical queries), and storage efficiency. Using TimescaleDB allows organizations to retain the benefits of PostgreSQL (SQL, relational model, ecosystem) while gaining the performance and features of a specialized time-series database. This effectively mitigates many of the limitations encountered when using vanilla PostgreSQL for high-scale device status management.
While a hybrid approach using different databases for different parts of a data pipeline is sometimes seen (e.g., one for ingest, another for analytics), for the core task of storing and providing access to device statuses, a single, consistent system of record is generally preferable to avoid data synchronization complexities and simplify application logic.
Guidance for Prototyping and Benchmarking
Given the nuanced trade-offs, it is strongly recommended that the user conduct their own benchmarks using realistic data shapes, volumes, and query patterns representative of their specific application. Generic benchmarks provide guidance, but workload-specific performance can vary significantly.
Key aspects to evaluate during prototyping and benchmarking include:
	•	Define Key Performance Indicators (KPIs): Establish clear metrics for ingestion throughput (records/second), query latency for critical query types (e.g., latest status, historical range scans, complex payload filters, aggregations), and storage footprint.
	•	Test High Cardinality Scenarios: Specifically simulate scenarios with a high number of unique device_id values to assess the impact on both MongoDB's metaField bucketing and PostgreSQL's indexing.
	•	Evaluate Ease of Development: Assess the development effort required to implement common data access patterns, data manipulation tasks, and analytical queries in each system. Consider the learning curve for the team.
	•	Assess Operational Tasks: Evaluate the complexity of setting up, configuring, monitoring, backing up, and scaling each solution. Consider the skills required for ongoing maintenance.
The long-term implications of data governance and schema evolution are also critical. MongoDB's inherent flexibility is advantageous for rapid development but can pose challenges for maintaining long-term data consistency and semantic understanding if not managed with discipline. PostgreSQL's more structured nature, even when accommodating flexible JSONB payloads, tends to enforce greater discipline from the outset. For device status data that might be subject to audits, used for regulatory compliance, or feed into critical business intelligence systems, the governance implications of each database choice are significant.
7. Conclusion
The decision to use either MongoDB Time-Series collections or PostgreSQL with GIN indexes for storing device statuses is a significant architectural choice with long-term implications. Both solutions are capable of managing time-series data but are optimized for different priorities and exhibit distinct trade-off profiles. There is no single "best" solution; the optimal choice is highly context-dependent.
MongoDB Time-Series collections offer a compelling option for applications already within the MongoDB ecosystem, particularly those that benefit from a document-centric model and require high flexibility for evolving status payloads. Its strengths lie in specialized internal optimizations for time-series data, such as automatic bucketing and columnar compression, which can lead to improved storage efficiency and good performance for append-heavy workloads. The built-in sharding capabilities provide a more straightforward path to horizontal write scalability compared to native PostgreSQL. However, these benefits come with significant limitations, including highly restricted update capabilities, secondary indexing only on the metaField, a lack of transactional support for writes to time-series collections, and the absence of several standard MongoDB features like Change Streams and Atlas Search on these specialized collections.
PostgreSQL, augmented with GIN indexes on JSONB columns, provides a robust and versatile solution, anchored by its strong ACID compliance, mature SQL engine, and the ability to seamlessly integrate device status data with other relational datasets through complex joins. The GIN index offers powerful and flexible querying capabilities directly into the attributes of JSONB payloads, which can be crucial for applications needing to filter or analyze based on diverse status characteristics. The primary weaknesses of using native PostgreSQL for high-volume time-series data include the write overhead associated with GIN index maintenance, the potential for index bloat requiring careful management, and the lack of built-in advanced time-series features like automatic time-based partitioning and specialized columnar compression.
The "best" choice hinges on a careful evaluation of specific requirements:
	•	If the primary drivers are schema flexibility for rapidly evolving payloads, a document-native development experience, and built-in horizontal write scaling within an existing MongoDB environment, and if the limitations of Time-Series collections (especially regarding updates, transactions, and secondary indexing on payloads) are acceptable, then MongoDB Time-Series is a strong contender.
	•	If paramount importance is placed on strong ACID consistency, the ability to perform complex SQL queries joining status data with other relational tables, and flexible, powerful querying of attributes within the status payload itself, then PostgreSQL with GIN indexes is a more suitable foundation.
For organizations leaning towards PostgreSQL due to its relational strengths but facing the challenges of very large-scale time-series data, the TimescaleDB extension is a highly recommended path. TimescaleDB effectively transforms PostgreSQL into a specialized time-series database, addressing many of the native limitations concerning partitioning, compression, specialized analytical functions, and data lifecycle management, often outperforming both vanilla PostgreSQL and MongoDB in time-series-specific benchmarks.
Ultimately, the decision will have ripple effects on application architecture, the required developer skill sets, and long-term operational practices. The database landscape is also dynamic; features that are limitations today may be addressed in future releases. However, core architectural philosophies are less likely to change dramatically. Therefore, thorough prototyping and workload-specific benchmarking are non-negotiable to validate assumptions and make an informed decision that best aligns with the unique demands of the device status management workload.
Works cited
1. Time Series Collection Limitations - Database Manual v8.0 ..., https://www.mongodb.com/docs/manual/core/timeseries/timeseries-limitations/ 2. Optimizing Array Queries With GIN Indexes in PostgreSQL | Timescale, https://www.timescale.com/learn/optimizing-array-queries-with-gin-indexes-in-postgresql 3. Overview of PostgreSQL indexing - DEV Community, https://dev.to/digitalpollution/overview-of-postgresql-indexing-lpi 4. Time Series Data Introduction | MongoDB | MongoDB, https://www.mongodb.com/resources/basics/time-series-data-analysis 5. Overcoming MongoDB Limitations with Fauna, https://fauna.com/blog/overcoming-mongodb-limitations-with-fauna 6. Index types supported in Amazon Aurora PostgreSQL and Amazon ..., https://aws.amazon.com/blogs/database/index-types-supported-in-amazon-aurora-postgresql-and-amazon-rds-for-postgresql-gin-gist-hash-brin/ 7. About Time Series Data - Database Manual v8.0 - MongoDB Docs, 
